{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNihPZSW+riRxgtKzTN2HNA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VM-Janani/Byte_Pair_Encoding/blob/main/BPE\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import re"
      ],
      "metadata": {
        "id": "dkPTRWPjcg6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##functin to perform initial vocabulary and find the word frequency\n"
      ],
      "metadata": {
        "id": "vnA1zjL3Kh8Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initial_vocabulary_and_word_freq(text):\n",
        "    # Tokenize the text by whitespace to get a list of words\n",
        "    corpus = text.split()\n",
        "\n",
        "    # Initialize an empty set to collect unique characters\n",
        "    vocabulary = set()\n",
        "\n",
        "    # Add unique characters from each word to the vocabulary\n",
        "    for word in corpus:\n",
        "        for char in word:\n",
        "            vocabulary.add(char)\n",
        "\n",
        "    # Add the end-of-word symbol to the vocabulary\n",
        "    vocabulary.add('</w>')\n",
        "\n",
        "    # Convert the set to a sorted list to maintain consistent order\n",
        "    sorted_vocabulary = sorted(vocabulary)\n",
        "\n",
        "    # Calculate word frequencies\n",
        "    word_freq = Counter()\n",
        "    for word in corpus:\n",
        "        word_seq = [char for char in word] + ['</w>']\n",
        "        word_str = ' '.join(word_seq)\n",
        "        word_freq[word_str] += 1\n",
        "\n",
        "    return sorted_vocabulary, word_freq"
      ],
      "metadata": {
        "id": "oWtoffm9c-pL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###BPE ALGORITHM (token learner)"
      ],
      "metadata": {
        "id": "FenH8J4LO5ud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def learn_bpe_vocabulary(text, num_merges):\n",
        "    # Get initial vocabulary and word frequency\n",
        "    vocabulary, word_freq = initial_vocabulary_and_word_freq(text)\n",
        "\n",
        "    print(\"Initial Word Frequency:\\n\", word_freq)\n",
        "    print(\"\\nInitial Vocabulary:\\n\", vocabulary)\n",
        "\n",
        "\n",
        "    merges = []\n",
        "\n",
        "    # Perform num_merges merges\n",
        "    for _ in range(num_merges):\n",
        "        # Count frequency of all adjacent pairs\n",
        "        pair_counts = Counter()\n",
        "        for word, freq in word_freq.items():\n",
        "            symbols = word.split()\n",
        "            for i in range(len(symbols) - 1):\n",
        "                pair_counts[(symbols[i], symbols[i + 1])] += freq\n",
        "\n",
        "        # Find the most frequent pair\n",
        "        if not pair_counts:\n",
        "            break\n",
        "        most_frequent_pair = max(pair_counts, key=pair_counts.get)\n",
        "\n",
        "        # Add the most frequent pair to merges\n",
        "        merges.append(most_frequent_pair)\n",
        "\n",
        "        # Merge the most frequent pair\n",
        "        new_symbol = ''.join(most_frequent_pair)\n",
        "        vocabulary.append(new_symbol)\n",
        "\n",
        "        # Replace all instances of the most frequent pair in the word_freq dictionary\n",
        "        new_word_freq = Counter()\n",
        "        for word, freq in word_freq.items():\n",
        "            new_word = []\n",
        "            symbols = word.split()\n",
        "            i = 0\n",
        "            while i < len(symbols):\n",
        "                if i < len(symbols) - 1 and (symbols[i], symbols[i + 1]) == most_frequent_pair:\n",
        "                    new_word.append(new_symbol)\n",
        "                    i += 2\n",
        "                else:\n",
        "                    new_word.append(symbols[i])\n",
        "                    i += 1\n",
        "            new_word_freq[' '.join(new_word)] += freq\n",
        "        word_freq = new_word_freq\n",
        "\n",
        "    return vocabulary"
      ],
      "metadata": {
        "id": "LTk8ABxvdFQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BPE token segmenter"
      ],
      "metadata": {
        "id": "DbYuHVQeTKUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_vocab(vocabulary):\n",
        "    # Initialize an empty list to collect merges\n",
        "    vocab_list = []\n",
        "\n",
        "    # Iterate through the vocabulary to collect merges\n",
        "    for token in vocabulary:\n",
        "        # Check if the token is not a single character and not '</w>'\n",
        "        if len(token) > 1 and token != '</w>':\n",
        "            # Add the token to the list\n",
        "            vocab_list.append(token)\n",
        "\n",
        "    return vocab_list"
      ],
      "metadata": {
        "id": "CYqJKnPs6np2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_end_of_word(sentence):\n",
        "    # Split the sentence into words\n",
        "    words = sentence.split()\n",
        "\n",
        "    # Add </w> to the end of each word\n",
        "    words_with_eow = [word + '</w>' for word in words]\n",
        "\n",
        "    # Join the words back into a sentence without spaces\n",
        "    segmented_sentence = ''.join(words_with_eow)\n",
        "\n",
        "    return segmented_sentence\n",
        "\n",
        "def segment_text_with_vocab(sentance, vocabulary):\n",
        "    # Sort vocabulary by length in descending order\n",
        "\n",
        "    vocabulary = prepare_vocab(vocabulary)\n",
        "    sorted_vocabulary = sorted(vocabulary, key=len, reverse=True)\n",
        "\n",
        "    # Add end-of-word symbol to the sentence\n",
        "    text = add_end_of_word(sentance)\n",
        "\n",
        "    # Initialize an empty string to store the segmented text\n",
        "    segmented_text = \"\"\n",
        "    i = 0\n",
        "\n",
        "    while i < len(text):\n",
        "        match = False\n",
        "        for token in sorted_vocabulary:\n",
        "            if text[i:i+len(token)] == token:\n",
        "                segmented_text += f\"({token}) \"\n",
        "                i += len(token)\n",
        "                match = True\n",
        "                break\n",
        "        if not match:\n",
        "            segmented_text += text[i]\n",
        "            i += 1\n",
        "\n",
        "    # Clean up extra spaces\n",
        "    segmented_text = segmented_text.replace(' </w>', '</w>')\n",
        "    tokens = segmented_text.split()\n",
        "    segmented_text = ''.join(tokens)\n",
        "\n",
        "    return segmented_text\n",
        "\n"
      ],
      "metadata": {
        "id": "YuExI_o34-JR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##EXAMPLE 1"
      ],
      "metadata": {
        "id": "Hn1_frsbR-9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "text = \"loud loud barking dog dog run run\"\n",
        "num_merges = 20\n",
        "\n",
        "vocabulary = learn_bpe_vocabulary(text, num_merges)\n",
        "print(\"\\nFinal Vocabulary:\\n\", vocabulary)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6BqtpsqdQ0O",
        "outputId": "e62ec2fd-41a5-4d81-ea57-a483a7f0b7a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Word Frequency:\n",
            " Counter({'l o u d </w>': 2, 'd o g </w>': 2, 'r u n </w>': 2, 'b a r k i n g </w>': 1})\n",
            "\n",
            "Initial Vocabulary:\n",
            " ['</w>', 'a', 'b', 'd', 'g', 'i', 'k', 'l', 'n', 'o', 'r', 'u']\n",
            "\n",
            "Final Vocabulary:\n",
            " ['</w>', 'a', 'b', 'd', 'g', 'i', 'k', 'l', 'n', 'o', 'r', 'u', 'g</w>', 'lo', 'lou', 'loud', 'loud</w>', 'do', 'dog</w>', 'ru', 'run', 'run</w>', 'ba', 'bar', 'bark', 'barki', 'barkin', 'barking</w>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"do dog run badly ba rung \"\n",
        "\n",
        "# Segment the sentence using the vocabulary\n",
        "segmented_sentence = segment_text_with_vocab(sentence, vocabulary)\n",
        "print(\"Segmented Sentence:\", segmented_sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYX3R-YNS4Xy",
        "outputId": "6455b274-aa88-4b2c-875c-fb0bbd4fcada"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmented Sentence: (do)</w>(dog</w>)(run</w>)(ba)dly</w>(ba)</w>(run)(g</w>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"lo run do dog bark rung loud lou\"\n",
        "\n",
        "# Segment the sentence using the vocabulary\n",
        "segmented_sentence = segment_text_with_vocab(sentence, vocabulary)\n",
        "print(\"Segmented Sentence:\", segmented_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHpIgWzE6llV",
        "outputId": "142c2b6f-9cb7-40e3-ecbf-1908df75c1fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmented Sentence: (lo)</w>(run</w>)(do)</w>(dog</w>)(bark)</w>(run)(g</w>)(loud</w>)(lou)</w>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##EXAMPLE 2"
      ],
      "metadata": {
        "id": "oPkVJzWPUoVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "text1 = \"good morning, it's a great morning to go out\"\n",
        "num_merges = 30\n",
        "\n",
        "vocabulary1 = learn_bpe_vocabulary(text1, num_merges)\n",
        "print(\"\\nFinal Vocabulary:\\n\", vocabulary1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9kgtAJ4Ut_N",
        "outputId": "8c006b2c-5451-4119-840e-f2effb065be2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Word Frequency:\n",
            " Counter({'g o o d </w>': 1, 'm o r n i n g , </w>': 1, \"i t ' s </w>\": 1, 'a </w>': 1, 'g r e a t </w>': 1, 'm o r n i n g </w>': 1, 't o </w>': 1, 'g o </w>': 1, 'o u t </w>': 1})\n",
            "\n",
            "Initial Vocabulary:\n",
            " [\"'\", ',', '</w>', 'a', 'd', 'e', 'g', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u']\n",
            "\n",
            "Final Vocabulary:\n",
            " [\"'\", ',', '</w>', 'a', 'd', 'e', 'g', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', 'go', 'mo', 'mor', 'morn', 'morni', 'mornin', 'morning', 't</w>', 'goo', 'good', 'good</w>', 'morning,', 'morning,</w>', 'it', \"it'\", \"it's\", \"it's</w>\", 'a</w>', 'gr', 'gre', 'grea', 'great</w>', 'morning</w>', 'to', 'to</w>', 'go</w>', 'ou', 'out</w>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"it's good to see you gopal more\"\n",
        "\n",
        "# Segment the sentence using the vocabulary\n",
        "segmented_sentence = segment_text_with_vocab(sentence, vocabulary1)\n",
        "print(\"Segmented Sentence:\", segmented_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2r6mRkYUty-",
        "outputId": "f842f86f-fdbf-4b4b-c908-ce4193f2a2d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmented Sentence: (it's</w>)(good</w>)(to</w>)see</w>y(ou)</w>(go)pal</w>(mor)e</w>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###index table method\n"
      ],
      "metadata": {
        "id": "NSukwoLZKS3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def segment_sentence(sentence, vocabulary):\n",
        "    # Add end-of-word symbol to the sentence\n",
        "    segmented_sentence = add_end_of_word(sentence)\n",
        "\n",
        "    vocabulary = prepare_vocab(vocabulary)\n",
        "\n",
        "    # Initialize the index table\n",
        "    index_table = []\n",
        "\n",
        "    # Iterate over each token in the vocabulary in the given order\n",
        "    for token in vocabulary:\n",
        "        # Initialize start index\n",
        "        start_idx = 0\n",
        "\n",
        "        # Iterate over the sentence to find and replace token occurrences\n",
        "        while start_idx < len(segmented_sentence):\n",
        "            # Find the next occurrence of the token\n",
        "            start_idx = segmented_sentence.find(token,start_idx)\n",
        "\n",
        "            # If token is not found, break out of the loop\n",
        "            if start_idx == -1:\n",
        "                break\n",
        "\n",
        "            # Calculate end index of the token\n",
        "            end_idx = start_idx + len(token)\n",
        "\n",
        "            # Remove any overlapping indices from the index table\n",
        "            index_table = [(start, end) for start, end in index_table if not (start >= start_idx and end <= end_idx)]\n",
        "\n",
        "            # Add the new indices to the table\n",
        "            index_table.append((start_idx, end_idx))\n",
        "\n",
        "            # Move start_idx forward to avoid infinite loop on overlapping tokens\n",
        "            start_idx += len(token)\n",
        "\n",
        "    # Sort the index table\n",
        "    index_table.sort()\n",
        "\n",
        "    # Construct the segmented sentence with parentheses\n",
        "    result = []\n",
        "    prev_end = 0\n",
        "\n",
        "    for start, end in index_table:\n",
        "        result.append(segmented_sentence[prev_end:start])\n",
        "        result.append('(' + segmented_sentence[start:end] + ')')\n",
        "        prev_end = end\n",
        "\n",
        "    result.append(segmented_sentence[prev_end:])\n",
        "    segmented_sentence = ''.join(result)\n",
        "\n",
        "    return segmented_sentence\n",
        "\n"
      ],
      "metadata": {
        "id": "ufKrG91H5haJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "sentence = \"do dog run badly ba rung\"\n",
        "segmented_sentence = segment_sentence(sentence, vocabulary)\n",
        "print(\"Segmented Sentence:\", segmented_sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3HEkdorWGIH",
        "outputId": "0f9c99f9-e9c7-4afd-8974-ea338dd7ccfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmented Sentence: (do)</w>(dog</w>)(run</w>)(ba)dly</w>(ba)</w>(run)(g</w>)\n"
          ]
        }
      ]
    }
  ]
}